{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/peremartramanonellas/template-with-sklearn-to-solve-any-class-problem?scriptVersionId=104390089\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Template to solve with minimum effort Regression & Classification Problems with SKLearn. \n\nI made this template to obtain a really **fast a first solution** for regression & classification problems with .csv or tabular datasets. \n\nIt's mainly based in two functions. \n* **data_transform**: This is the one responsible to modify the dataset, and transform the data to do it usable for the model. \n * *Null values*: By the moment only replace with the most used value in the column. \n * *Non numeric values*: At this moment stops the treatment. In this versiÃ³n the function only accepts numeric columns. \n * *Normalize Data*.: It's possible to indicate a maximum standard desviation, and the function normalize the columns with a std bigger than the one indicated \n \n* **create_model**: I use RandomizedSearchCV to test different hyperparametres, and SKlearn selects the best one. \n\nOf course that it's impossible to obtain the best solution to all classfication & regression problems with this template, but is a simple place where to start, study the results and continue with more advanced tunning. \n\nMy intention is improve the data_transform function, not only to transform data but to have a fast way to obtain information of the datasets and save time. \n\nFeel free to Copy & Edit, or fork this Notebook and use at your convenience, just please upvote the notebbok if you like and use it.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-27T17:17:13.692629Z","iopub.execute_input":"2022-08-27T17:17:13.693101Z","iopub.status.idle":"2022-08-27T17:17:15.392923Z","shell.execute_reply.started":"2022-08-27T17:17:13.693006Z","shell.execute_reply":"2022-08-27T17:17:15.391589Z"}}},{"cell_type":"markdown","source":"## The dataset\nI used the Credit Fraud Detection Dataset. \nhttps://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\nIt's a really imbalanced Dataset with a small representation of fraud cases. In this kind of datasets the accuracy is a useless metric. I did nothing to solve the problme of imbalanced. \n\nIn future notebooks I will try with different Datasets. ","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:20.650433Z","iopub.execute_input":"2022-08-28T13:23:20.65121Z","iopub.status.idle":"2022-08-28T13:23:22.612193Z","shell.execute_reply.started":"2022-08-28T13:23:20.651062Z","shell.execute_reply":"2022-08-28T13:23:22.610727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support functions. ","metadata":{}},{"cell_type":"code","source":"def evaluate_regression(y_true, y_preds):\n    from sklearn.metrics import r2_score\n\n    r2_score = r2_score(y_preds, y_true)\n\n    metric_dict = {\"r2_score\": round(r2_score, 2)}\n    print(f\"KPIs-------------------------------------\")\n    print(f\"r2: {r2_score * 100:.2f}%\")\n    print(f\"KPIs-------------------------------------\")\n    return metric_dict","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:22.615967Z","iopub.execute_input":"2022-08-28T13:23:22.616437Z","iopub.status.idle":"2022-08-28T13:23:22.623346Z","shell.execute_reply.started":"2022-08-28T13:23:22.616393Z","shell.execute_reply":"2022-08-28T13:23:22.622139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_preds):\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n    accuracy = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds)\n    recall = recall_score(y_true, y_preds)\n    f1 = f1_score(y_true, y_preds)\n    metric_dict = {\"accuracy\": round(accuracy, 2), \n                  \"precision\":  round(accuracy, 2),\n                  \"recall\": round(recall, 2),\n                  \"f1\": round(f1, 2)}\n    print(f\"KPIs-------------------------------------\")\n    print(f\"Acc: {accuracy * 100:.2f}%\")\n    print(f\"precision: {precision * 100:.2f}%\")\n    print(f\"recall: {recall * 100:.2f}%\")\n    print(f\"f1score: {f1 * 100:.2f}%\")\n    print(f\"KPIs-------------------------------------\")\n    return metric_dict","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:22.625335Z","iopub.execute_input":"2022-08-28T13:23:22.62614Z","iopub.status.idle":"2022-08-28T13:23:22.638855Z","shell.execute_reply.started":"2022-08-28T13:23:22.626097Z","shell.execute_reply":"2022-08-28T13:23:22.637142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_confusion_matrix(y_true, y_preds):\n    cm = pd.crosstab(y_true, y_preds, rownames=['Actual'], colnames=['Predicted'])\n    fig, (ax1) = plt.subplots(ncols=1, figsize=(8,8))\n    sns.heatmap(cm, \n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Oranges\")\n    plt.title('Confusion Matrix', fontsize=14)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:22.64271Z","iopub.execute_input":"2022-08-28T13:23:22.643702Z","iopub.status.idle":"2022-08-28T13:23:22.654206Z","shell.execute_reply.started":"2022-08-28T13:23:22.643631Z","shell.execute_reply":"2022-08-28T13:23:22.652388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data_transform\nTransform the datased to do it usable in a Machine Learning Model. \n* **pd_dataframe**: The dataframe to check / transform. \n* **normalize**: Indicate when we want normalize. \n* **stdlimit**: If normalize is True, will normalize the columns with a *std* > to this parameter. ","metadata":{}},{"cell_type":"code","source":"def data_transform(pd_dataframe, normalize=False, stdlimit=2):\n    \n    colums_2_transform = []\n    newdf = pd_dataframe.copy()\n    pretestfail=False\n    \n    #Pre checks.    \n    #This versiond don't convert Categories to Columns. TBD. \n    for dttype in dataframe.dtypes:\n        if dttype == 'object':\n            print ('Please, only floats or ints')\n            pretestfail = True\n            \n    #This version have only a treatment for null values. TDB. \n    if (dataframe.isnull().sum().max()) > 0 and (not pretestfail) :\n        #to be done\n        pd_dataframe = pd_dataframe.apply(lambda x:x.fillna(x.value_counts().index[0]))\n        #print (\"You must do something with the nulls values before ;-)\")\n        pretestfail = True\n    \n    if pretestfail: \n        return nwdf\n\n    #Normalize the values. TBD: Add more ways to change the data. \n    if normalize: \n        for n in range(len(pd_dataframe.columns)): \n            #std = pd_dataframe.take([n], axis=1).describe().loc[['std']]\n            std = pd_dataframe.take([n], axis=1).describe().loc[['std', 'min', 'max']]\n            if float(std.iloc[0]) > stdlimit: \n                column = pd_dataframe.columns[n]\n                colums_2_transform.append(column)\n                min = float(std.iloc[1])\n                max = float(std.iloc[2])\n                print ('min:', min)\n                print ('max:', max)\n                newdf[column] = (pd_dataframe[column] - min) / max - min\n        print (\"Columns to normalize: \", colums_2_transform)\n    \n    return newdf","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:22.657117Z","iopub.execute_input":"2022-08-28T13:23:22.658189Z","iopub.status.idle":"2022-08-28T13:23:22.675134Z","shell.execute_reply.started":"2022-08-28T13:23:22.658139Z","shell.execute_reply":"2022-08-28T13:23:22.673622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## create_model\nI use a grid to create a hyperparameters combination and RandomizedSearchCV to try different combinations. I use a small number of cross validation to reduce the training time, but it can be increased if training time is not relevant.\n\n* **data**: The dataframe used. \n* **target**: The name of the Target Column. \n* **iterations**: How many combinations we want to try. \n* **alg**: 0 for RandomForestClassifier, 1 for RandomForestRegressor. \n\n","metadata":{}},{"cell_type":"code","source":"def create_model(data, target, iterations, alg=0):\n    np.random.seed(50)\n    \n    pd_dataframe = data\n    X = pd_dataframe.drop(target, axis=1)\n    y = pd_dataframe[target]\n    \n    #split data \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n \n    #configure values hyperparameters\n    gscgrid = {'n_estimators': [10, 50, 100, 120, 150, 400], \n       'max_depth': [None, 5, 10, 20, 100], \n       'min_samples_split': [2, 4, 6, 12], \n       'min_samples_leaf': [1, 2, 4, 6, 12]}\n    if alg == 0:\n        model = RandomForestClassifier(n_jobs = 1)\n    else: \n        model = RandomForestRegressor(n_jobs = 1)\n    \n    #using RandomizedSearchCV to try different hyperparameters\n    #cv is crossvalidation, the default value is 5. \n    #verbose indicates the level of trace desired. \n    #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n    gscmodel = RandomizedSearchCV(estimator=model, \n                           param_distributions=gscgrid, \n                           n_iter=iterations, \n                           cv=2, \n                           verbose=2)\n    gscmodel.fit(X_train, y_train)\n    y_preds = gscmodel.predict(X_test)\n    \n    if alg == 0: \n        evaluate (y_test, y_preds)\n    else: \n        evaluate_regression(y_test, y_preds)\n    \n    return gscmodel, X_train, X_test, y_train, y_test, y_preds","metadata":{"execution":{"iopub.status.busy":"2022-08-28T14:11:14.668779Z","iopub.execute_input":"2022-08-28T14:11:14.669343Z","iopub.status.idle":"2022-08-28T14:11:14.681667Z","shell.execute_reply.started":"2022-08-28T14:11:14.669301Z","shell.execute_reply":"2022-08-28T14:11:14.680337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# USE THE TEMPLATE AND SOLVE THE PROBLEM","metadata":{}},{"cell_type":"code","source":"dataframe = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\ndataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:22.696355Z","iopub.execute_input":"2022-08-28T13:23:22.699428Z","iopub.status.idle":"2022-08-28T13:23:27.859053Z","shell.execute_reply.started":"2022-08-28T13:23:22.699375Z","shell.execute_reply":"2022-08-28T13:23:27.857716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Call the data_transform\nI'm indicating a *std* limit of 5, any column with a *std*  bigger than five will be transformed.","metadata":{}},{"cell_type":"code","source":"pd_dataframe = data_transform(dataframe, normalize=1, stdlimit=5)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:27.860985Z","iopub.execute_input":"2022-08-28T13:23:27.861388Z","iopub.status.idle":"2022-08-28T13:23:28.619059Z","shell.execute_reply.started":"2022-08-28T13:23:27.861354Z","shell.execute_reply":"2022-08-28T13:23:28.617564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#There are only 492 frauds. \npd_dataframe['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:28.620917Z","iopub.execute_input":"2022-08-28T13:23:28.621947Z","iopub.status.idle":"2022-08-28T13:23:28.636413Z","shell.execute_reply.started":"2022-08-28T13:23:28.621894Z","shell.execute_reply":"2022-08-28T13:23:28.634874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function has transformed Two columns. *Time* and *Data*. We can see in the result of the *head* function that now the values in *Time* and *Amount* are totaly different and between 0 and 1. ","metadata":{}},{"cell_type":"code","source":"pd_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T13:23:28.640376Z","iopub.execute_input":"2022-08-28T13:23:28.640824Z","iopub.status.idle":"2022-08-28T13:23:28.67801Z","shell.execute_reply.started":"2022-08-28T13:23:28.640769Z","shell.execute_reply":"2022-08-28T13:23:28.676634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Call the create_model\n\nHere you can change the parameters to adapt to your model. \n* data: your .csv, previously treated, without empty data and only numbers. \n* target: the dependent variable. \n* iterations: more iterations more hyperparametrers configurations will try","metadata":{}},{"cell_type":"code","source":"#Algorithms that can be used by the create_model function. \nCLASSIFICATION=0\nREGRESSION=1\n\n#create_model returns a tupla of values, with this constants you can acces \n#to the data inside the collection. \nMODEL = 0\nXTRAIN = 1 \nXTEST =2\nYTRAIN =3\nYTEST =4\nYPREDS = 5\n\nmodel = create_model(data=pd_dataframe, \n                     target = 'Class', \n                     iterations=15, \n                    alg = CLASSIFICATION)\nmodel[MODEL].best_params_","metadata":{"execution":{"iopub.status.busy":"2022-08-28T14:11:21.222301Z","iopub.execute_input":"2022-08-28T14:11:21.223696Z","iopub.status.idle":"2022-08-28T15:10:01.080212Z","shell.execute_reply.started":"2022-08-28T14:11:21.223651Z","shell.execute_reply":"2022-08-28T15:10:01.078614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#In our test data we have 88 frauds. \nmodel[YTEST].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:32.674737Z","iopub.execute_input":"2022-08-28T15:10:32.675247Z","iopub.status.idle":"2022-08-28T15:10:32.686808Z","shell.execute_reply.started":"2022-08-28T15:10:32.675202Z","shell.execute_reply":"2022-08-28T15:10:32.685448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_confusion_matrix(model[YTEST], model[YPREDS])","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:35.229946Z","iopub.execute_input":"2022-08-28T15:10:35.230429Z","iopub.status.idle":"2022-08-28T15:10:35.557206Z","shell.execute_reply.started":"2022-08-28T15:10:35.230392Z","shell.execute_reply":"2022-08-28T15:10:35.556252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see in the Confusion Matrix, we are able to detect 67 of 88 frauds. It's good?.... who knows. But our system have an xxx of accuracy, really amaziong! But, how I said just at the beginning the accuracy dosn't work with imbalanced data. We can fail all the frauds detections and obtain an fantastic accuracy. \n\n**Use always a confusion matrix to check the results in datasets with imbalanced data**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}